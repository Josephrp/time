*neural network status.

Opposite modes or states :

diffuse, confused,       chaotic.
focused, clear thinking, orderly.

We can think of this as the result of a projection with target accuracy and precision, like
a data type selection.

so we can think of the ooda loop as having many such functions
each of them moving biased local subjective samples of the world around.

now consider models of these workflows, the compiler is an embodiment of
one such model.

The idea of a program being a generic unitary object, like a UU in HOTT,
or a clifford algebra, a set of bits, each having a meaning, where even the number of
bits is calculated by propositions, and being abstract and related
to dsl, transported in TLS streams or p2p messages,
as files in an open source os.

Now consider the coq and metacoq applications as higher order programs.

So we have 8 levels of unitary groups.

bott periodicity theorem, applied to deep graph neural nework embeddings, or continious deformations of asts of programs, or algebraic structurs, formalized from word problems or narratives, structured with meta narratives, connected with performance metrics and observability, in an cybernetic OODA loop.

https://github.com/meta-introspector/meta-meme/issues/152


* models

vault creates key
key creates cluster 
iso image created
network topologies created.
instance boot from iso image and user data script loads to boot.
docker software installed from package repo
git repo pulled
docker image pulled from docker repo
inference applied from model blocks to records extracted from datasets.
configuration management
performance data collected
usage data collected
user profiles collected
programming language packages
incidents and changes, patches applied
audits 
reports, dialogs, emails, chats, meetings.
datasets created from all the other sources.

* layers

** private data
like server keys

private git hub repos on aws code commit for usage by servers.
with aws type, account, region, repo and branch or commit
parameters in ssm, or consul, or vault

property like servers and buildings
profile data
user profiles
traffic samples

** protected data
like cluster configurations
parameters in ssm, or consul, or vault

** public data
like protocols and open source code.

a bias towards freedom.
ontologies, schemas and forms.

* generators

Imagine a terraform script that
generates a terraform script and
runs it on a new aws account it creates.
it can be used to provision runners
of a single command and audits
the execution.
like atlantis but self hosting
in docker.
it would collect perf data, systrace,
protocols, etc.
it would generate minimal access policies
for the run system and extensive reports.
we can feed those to ai with priority.

