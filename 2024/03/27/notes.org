
 1728  cd guix-cuirass/
 1733  sudo chown -R cuirass: /etc/guix/*
 1736  guix pull
 1772  cd guix-cuirass
 1831  guix search nrpe
 1876  guix install python-pytorch
 1888  guix shell -D 
 1988  guix install grpc_tools
 1989  guix install grpc
 1990  guix shell 
 guix shell -D protobuf grpc python-pytorch

imagine converting bash command into scm file

I ran:
~/2024/03/27/hivemind/hivemind/proto/generate.sh

python  -m grpc_tools.protoc --proto_path=. --python_out=.  auth.proto averaging.proto crypto.proto dht.proto p2pd.proto runtime.proto test.proto 


Installing collected packages: speedtest-cli, sentencepiece, mpmath, cpufeature, bitsandbytes, urllib3, typing-extensions, tqdm, sympy, six, safetensors, regex, pyyaml, psutil, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, idna, humanfriendly, fsspec, filelock, charset-normalizer, certifi, async-timeout, triton, requests, pydantic, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, Dijkstar, nvidia-cusolver-cu12, huggingface-hub, torch, tokenizers, transformers, accelerate, tensor-parallel, peft, petals

python -m petals.cli.run_server --port 31330 petals-team/StableBeluga2




pip install torch~=2.2.0 torch_xla[tpu]~=2.2.0 -f https://storage.googleapis.com/libtpu-releases/index.html


python -m petals.cli.run_server --port 31330 petals-team/StableBeluga2 --device xla --num_blocks=10

PJRT_DEVICE=TPU python -m petals.cli.run_server --port 31330 petals-team/StableBeluga2 --device xla --num_blocks=10
